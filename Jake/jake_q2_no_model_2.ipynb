{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the CNN with varying input patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the performance of the CNN by varying the amplitude A of the patterns while keeping DX fixed, namely by changing the signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalise setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten #, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "L=60\n",
    "input_shape = (L, 1)\n",
    "n_class = 3\n",
    "BATCH_SIZE = 250\n",
    "EPOCHS = 100\n",
    "accuracy_1 =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jump = lambda drift, stdev: int(np.random.normal(drift,stdev))\n",
    "\n",
    "def pattern(i,z,a):\n",
    "    p = a*np.sin((np.pi*i)/z)\n",
    "    return p.astype(int) #Use numpy convesion instead it aviods \"IOPub data rate exceeded\"\n",
    "\n",
    "def gen_data(A=500): #Generate datasets with varying A     \n",
    "    np.random.seed(12345)\n",
    "    Z=12 # Z=nr of steps\n",
    "    N=10000  # number of data samples\n",
    "    L=60 # size of each sample of the timeseries \n",
    "    DX = 50  # step parameters: introduce small positive bias \n",
    "    bias = 5\n",
    "\n",
    "    y = [0] * N\n",
    "    x = [[0] * L for i in range(N)] \n",
    "    for i in range(N):\n",
    "        if i>0:\n",
    "            x[i][0] = x[i-1][-1] + jump(bias,DX)\n",
    "        for j in range(1,L):\n",
    "            x[i][j] = x[i][j-1] + jump(bias,DX)  \n",
    "        y[i] = i%3 \n",
    "        if y[i]>0:\n",
    "            j0 = np.random.randint(0,L-1-Z)\n",
    "            sign = 3-2*y[i]\n",
    "            for j in range(Z):\n",
    "                x[i][j0+j] += sign*pattern(j,Z,A)\n",
    "    return np.asarray(x), np.asarray(y)\n",
    "\n",
    "def prep_data(x,y):\n",
    "    xm = x.mean(axis=1)\n",
    "    for i in range(N):\n",
    "        x[i] = x[i]-xm[i]\n",
    "\n",
    "    x = x/400\n",
    "    \n",
    "    #spliting of training and validation \n",
    "    perc_train=0.8 \n",
    "    N_train = int(perc_train*N)\n",
    "    x_train = x[:N_train]\n",
    "    y_train = y[:N_train]\n",
    "    x_val = x[N_train:]\n",
    "    y_val = y[N_train:]\n",
    "\n",
    "    # Keras wants an additional dimension with a 1 at the end\n",
    "    x_train = x_train.reshape(x_train.shape[0], L, 1)\n",
    "    x_val =  x_val.reshape(x_val.shape[0], L, 1)\n",
    "    input_shape = (L, 1)\n",
    "    return x_train, y_train, x_val, y_val, input_shape\n",
    "\n",
    "def build_model(NCONV=1):\n",
    "    reg = tf.keras.regularizers.l2(0.2) ## raised to 0.2\n",
    "    #reg = regularizers.l1(0.1)\n",
    "    ini = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, \n",
    "                                            seed=None)\n",
    "    NCONV = 1\n",
    "    NF = 5\n",
    "    # challenge: at most 600 tunable parameters\n",
    "    model = Sequential()\n",
    "    if NCONV==1:\n",
    "        # -----2-----\n",
    "        model.add(Conv1D(filters=NF, kernel_size=11, \n",
    "                         kernel_initializer=ini, \n",
    "                         kernel_regularizer=reg,\n",
    "                         activation='relu', \n",
    "                         input_shape=input_shape))\n",
    "        #model.add(MaxPooling1D(3))\n",
    "        model.add(AveragePooling1D(5))\n",
    "        model.add(Conv1D(filters=5, kernel_size=7, \n",
    "                         activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    if NCONV==2:\n",
    "        # -----1-----\n",
    "        model.add(Conv1D(filters=NF, kernel_size=11, \n",
    "                         kernel_initializer=ini, \n",
    "                         kernel_regularizer=reg, ######## TRY WITHOUT !\n",
    "                         activation='relu', input_shape=input_shape))\n",
    "\n",
    "        model.add(AveragePooling1D(5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(9, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(6, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model2(model):\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    reg2 = tf.keras.regularizers.l1(0.5)\n",
    "    model2 = keras.models.clone_model(model) #based on model1\n",
    "    model2.layers[0].kernel_regularizer = reg2\n",
    "    model2.layers[2].kernel_regularizer = reg2\n",
    "    model2.compile(loss=keras.losses.categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model2\n",
    "\n",
    "def show_confusion_matrix(validations, predictions, label=\"Model\"):\n",
    "    LABELS = [\"absent\",\"positive\",\"negative\"]\n",
    "    cmap=\"GnBu\"\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    seaborn.heatmap(matrix,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                cmap=cmap)\n",
    "    plt.title(label+': Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def Show_history(fit, label =''):\n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,5.))\n",
    "    ax=AX[0]\n",
    "    ax.set_title(label+'')\n",
    "    ax.plot(fit.history['accuracy'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_accuracy'],\"r--\",label=\"valid.\")\n",
    "    ax.plot((0,EPOCHS),(1/3,1/3),\":\",c=\"gray\",label=\"random choice\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax=AX[1]\n",
    "    ax.set_title(label+' Loss ')\n",
    "    ax.plot(fit.history['loss'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_loss'],\"r--\",label=\"valid.\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim([0, 1.05*np.max(fit.history['loss'])])\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and test varying pattern amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  0\n",
      "A =  50\n",
      "A =  100\n",
      "A =  150\n",
      "A =  200\n",
      "A =  250\n",
      "A =  300\n",
      "A =  350\n",
      "A =  400\n",
      "A =  450\n",
      "A =  500\n",
      "A =  550\n",
      "A =  600\n",
      "A =  650\n",
      "A =  700\n",
      "A =  800\n",
      "[0.33899998664855957, 0.3400000035762787, 0.36149999499320984, 0.40950000286102295, 0.5090000033378601, 0.6209999918937683, 0.7235000133514404, 0.8015000224113464, 0.8675000071525574, 0.9104999899864197, 0.9434999823570251, 0.9629999995231628, 0.9754999876022339, 0.9835000038146973, 0.9904999732971191, 0.9934999942779541]\n"
     ]
    }
   ],
   "source": [
    "Amplitudes  =[0,50,100,150,200,250,300,350,400,450,500,550,600,650,700,800]\n",
    "model = build_model()\n",
    "\n",
    "for A in Amplitudes:\n",
    "    print('A = ', A)\n",
    "    x, categ = gen_data(A)\n",
    "    n_class = 3\n",
    "    N,L = len(x), len(x[0])\n",
    "    y = np.zeros((N,n_class))\n",
    "    for i in range(N):\n",
    "        y[i][categ[i]] = 1. #prof does it here but might add to gen_data \n",
    "    x_train, y_train, x_val, y_val, input_shape = prep_data(x,y)\n",
    "    fit = model.fit(x_train,y_train,batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0, shuffle=True)  \n",
    "    \n",
    "    loss1, acc1 = model.evaluate(x=x_val, y=y_val, verbose = 0)\n",
    "    accuracy_1.append(acc1)\n",
    "    y_pred_val = model.predict(x_val)\n",
    "    # Take the class with the highest probability from the val predictions\n",
    "    max_y_pred_val = np.argmax(y_pred_val, axis=1)\n",
    "    max_y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "    #show_confusion_matrix(max_y_val, max_y_pred_val, label=\"Model 1, A = \"+str(A))\n",
    "    \n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(Amplitudes, accuracy_1, label =\"Model 1\")\n",
    "#ins = ax.inset_axes([0.57,0.18,0.4,0.4])\n",
    "ax.set_xlabel(\"Amplitude, A\")\n",
    "ax.set_ylabel(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
