{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc56b38",
   "metadata": {},
   "source": [
    "# Initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab88786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten #, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "\n",
    "L=60\n",
    "input_shape = (L, 1)\n",
    "n_class = 3\n",
    "BATCH_SIZE = 250\n",
    "EPOCHS = 100\n",
    "accuracy_1 =[]\n",
    "\n",
    "def Show_data(x,L,s=\"data\"):\n",
    "    plt.plot(np.arange(L),x[0])\n",
    "    plt.plot(np.arange(L,2*L),x[1])\n",
    "    plt.plot(np.arange(2*L,3*L),x[2])\n",
    "    plt.title(s)\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.show()\n",
    "    \n",
    "def Show_weights(model,l=0,label=\"model\"):\n",
    "    c=['r','y','c','b','m'] #colori\n",
    "    m=['o','s','D','<','>'] #markers\n",
    "    ms=10\n",
    "    \n",
    "    w = model.layers[l].get_weights()[0]\n",
    "    wT=w.T\n",
    "    M=len(wT)\n",
    "    b = model.layers[l].get_weights()[1]\n",
    "    \n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,4.4))\n",
    "    ax=AX[0]\n",
    "    ax.axhline(0, c=\"k\")\n",
    "    ax.plot((0,))\n",
    "    for i in range(M):\n",
    "        ax.plot(wT[i][0],\"-\",c=c[i],marker=m[i],label=str(i),markersize=ms)\n",
    "    ax.set_title(label+': filters of layer '+str(l))\n",
    "    ax.set_xlabel('index')\n",
    "    ax=AX[1]\n",
    "    ax.axhline(0, c=\"k\")\n",
    "    for i in range(M):\n",
    "        ax.plot((i),(b[i]),c=c[i],marker=m[i],label=\"filter \"+str(i),markersize=ms)\n",
    "    ax.set_title(label+': bias of layer '+str(l))\n",
    "    ax.set_xlabel('filter nr')\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "def Show_history(fit):\n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,5.))\n",
    "    ax=AX[0]\n",
    "    ax.plot(fit.history['accuracy'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_accuracy'],\"r--\",label=\"valid.\")\n",
    "    ax.plot((0,EPOCHS),(1/3,1/3),\":\",c=\"gray\",label=\"random choice\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax=AX[1]\n",
    "    ax.plot(fit.history['loss'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_loss'],\"r--\",label=\"valid.\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim([0, 1.05*np.max(fit.history['loss'])])\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def build_model(init='normal', reg='l2', lmb=0.2):\n",
    "    #Initializers\n",
    "    if init=='zeros':\n",
    "        ini=tf.keras.initializers.Zeros()\n",
    "    elif init=='ones':\n",
    "        ini=tf.keras.initializers.Ones()\n",
    "    elif init=='normal':\n",
    "        ini=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    else:\n",
    "        print('Error Initializers')\n",
    "    #Regularizers\n",
    "    if reg=='l1':\n",
    "        rer=tf.keras.regularizers.l1(lmb)\n",
    "    elif reg=='l2':\n",
    "        reg=tf.keras.regularizers.l2(lmb)\n",
    "    elif reg=='l1_l2':\n",
    "        reg=tf.keras.regularizers.l1_l2(lmb, lmb)\n",
    "    else:\n",
    "        print('Error Regularizers')\n",
    "    # challenge: at most 600 tunable parameters\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=5, kernel_size=11, \n",
    "                     kernel_initializer=ini, # ini - ini_0 - ini_1\n",
    "                     kernel_regularizer=reg,\n",
    "                     activation='relu', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(AveragePooling1D(5))\n",
    "    model.add(Conv1D(filters=5, kernel_size=7, \n",
    "                     activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_class, activation='softmax')) # softmax !    --->n_class=3 : Multiclassification\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "jump = lambda drift, stdev: int(np.random.normal(drift,stdev))\n",
    "\n",
    "def pattern(i,z,a):\n",
    "    p = a*np.sin((np.pi*i)/z)\n",
    "    return p.astype(int) #Use numpy convesion instead it aviods \"IOPub data rate exceeded\"\n",
    "\n",
    "def gen_data(A=500): #Generate datasets with varying A     \n",
    "    np.random.seed(12345)\n",
    "    Z=12 # Z=nr of steps\n",
    "    N=10000  # number of data samples\n",
    "    L=60 # size of each sample of the timeseries \n",
    "    DX = 50  # step parameters: introduce small positive bias \n",
    "    bias = 5\n",
    "\n",
    "    y = [0] * N\n",
    "    x = [[0] * L for i in range(N)] \n",
    "    for i in range(N):\n",
    "        if i>0:\n",
    "            x[i][0] = x[i-1][-1] + jump(bias,DX)\n",
    "        for j in range(1,L):\n",
    "            x[i][j] = x[i][j-1] + jump(bias,DX)  \n",
    "        y[i] = i%3 \n",
    "        if y[i]>0:\n",
    "            j0 = np.random.randint(0,L-1-Z)\n",
    "            sign = 3-2*y[i]\n",
    "            for j in range(Z):\n",
    "                x[i][j0+j] += sign*pattern(j,Z,A)\n",
    "    return np.asarray(x), np.asarray(y)\n",
    "\n",
    "def prep_data(x,y):\n",
    "    xm = x.mean(axis=1)\n",
    "    for i in range(N):\n",
    "        x[i] = x[i]-xm[i]\n",
    "\n",
    "    x = x/400\n",
    "    \n",
    "    #spliting of training and validation \n",
    "    perc_train=0.8 \n",
    "    N_train = int(perc_train*N)\n",
    "    x_train = x[:N_train]\n",
    "    y_train = y[:N_train]\n",
    "    x_val = x[N_train:]\n",
    "    y_val = y[N_train:]\n",
    "\n",
    "    # Keras wants an additional dimension with a 1 at the end\n",
    "    x_train = x_train.reshape(x_train.shape[0], L, 1)\n",
    "    x_val =  x_val.reshape(x_val.shape[0], L, 1)\n",
    "    input_shape = (L, 1)\n",
    "    return x_train, y_train, x_val, y_val, input_shape\n",
    "\n",
    "def show_confusion_matrix(validations, predictions, label=\"Model\"):\n",
    "    LABELS = [\"absent\",\"positive\",\"negative\"]\n",
    "    cmap=\"GnBu\"\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    seaborn.heatmap(matrix,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                cmap=cmap)\n",
    "    plt.title(label+': Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def Show_history(fit, label =''):\n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,5.))\n",
    "    ax=AX[0]\n",
    "    ax.set_title(label+'')\n",
    "    ax.plot(fit.history['accuracy'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_accuracy'],\"r--\",label=\"valid.\")\n",
    "    ax.plot((0,EPOCHS),(1/3,1/3),\":\",c=\"gray\",label=\"random choice\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax=AX[1]\n",
    "    ax.set_title(label+' Loss ')\n",
    "    ax.plot(fit.history['loss'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_loss'],\"r--\",label=\"valid.\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim([0, 1.05*np.max(fit.history['loss'])])\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d58e2",
   "metadata": {},
   "source": [
    "# Signal Amplitude\n",
    "\n",
    "With a greater amplitude for the sine pattern that is injected into the samples we are increasing the signal to noise ratio. This should make it easier for the CNN to detect the pattern. Here the intention is to quantify the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "Amplitudes  =[0,50,100,150,200,250,300,350,400,450,500,550,600,650,700,800]\n",
    "model = build_model()\n",
    "\n",
    "for A in Amplitudes:\n",
    "    print('A = ', A)\n",
    "    x, categ = gen_data(A)\n",
    "    n_class = 3\n",
    "    N,L = len(x), len(x[0])\n",
    "    y = np.zeros((N,n_class))\n",
    "    for i in range(N):\n",
    "        y[i][categ[i]] = 1. #prof does it here but might add to gen_data \n",
    "    x_train, y_train, x_val, y_val, input_shape = prep_data(x,y)\n",
    "    fit = model.fit(x_train,y_train,batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0, shuffle=True)  \n",
    "    \n",
    "    loss1, acc1 = model.evaluate(x=x_val, y=y_val, verbose = 0)\n",
    "    accuracy_1.append(acc1)\n",
    "    y_pred_val = model.predict(x_val)\n",
    "    # Take the class with the highest probability from the val predictions\n",
    "    max_y_pred_val = np.argmax(y_pred_val, axis=1)\n",
    "    max_y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "    #show_confusion_matrix(max_y_val, max_y_pred_val, label=\"Model 1, A = \"+str(A))\n",
    "    \n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd837bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(Amplitudes, accuracy_1, label =\"Model 1\")\n",
    "#ins = ax.inset_axes([0.57,0.18,0.4,0.4])\n",
    "ax.set_xlabel(\"Amplitude, A\")\n",
    "ax.set_ylabel(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04ecac",
   "metadata": {},
   "source": [
    "# Regularisation Tuning\n",
    "\n",
    "Regularisation restricts the size of the weights, thus the weights are less flexible and cannot fit the training data as well. This however means the model is more likley to follow the trend of the entire population. Thus regularisation can help validation accuracy improve. With too much regularisation the weights are too restriced and they can't fit the training data very well at all and thus don't follow the trend of the population either. Too much regularisation can make the validation accuracy worse.\n",
    "\n",
    "To achieve regularisation the function that guides the gradient descent is a compination of the cost function and a regularisation function. The parameter D tunes the amount of regularisation. With smaller D there is more regularisation. \n",
    "\n",
    "argmin($D$ Cost + Regularisation)\n",
    "\n",
    "There are multiple possible choices for the regularisation function. The three ones provided by keras are tested l1,l2 and l1_l2.\n",
    "\n",
    "How to find the best combination of parameters? Well with a grid search of course..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578ba96",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "str0 = 'ts_L60_Z12_A500_DX50_bias5_N10000.dat'\n",
    "fnamex='DATA/x_'+str0\n",
    "fnamey='DATA/y_'+str0\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "N,L = len(x), len(x[0]) #10000 60\n",
    "Show_data(x,L,\"original data\")\n",
    "categ = np.loadtxt(fnamey, dtype=int)\n",
    "n_class = 3    # y.argmax() - y.argmin() +1\n",
    "print('data: ',N)\n",
    "y = np.zeros((N,n_class))\n",
    "for i in range(N):\n",
    "    y[i][categ[i]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff45d1",
   "metadata": {},
   "source": [
    "### Split and Rescale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIOIN 1----- \n",
    "xm = x.mean(axis=1)\n",
    "for i in range(N):\n",
    "    x[i] = x[i]-xm[i]\n",
    "x_scaled = x/400\n",
    "Show_data(x_scaled,L,\"rescaled data\") \n",
    "\n",
    "perc_train=0.8\n",
    "N_train = int(perc_train*N)\n",
    "x_train = x_scaled[:N_train]\n",
    "y_train = y[:N_train]\n",
    "x_val = x_scaled[N_train:]\n",
    "y_val = y[N_train:]\n",
    "N_val = len(x_val)\n",
    "print('N_train=',N_train,'  N_val=',N_val,'  L=',L,'  n_class=',n_class)\n",
    "# Keras wants an additional dimension with a 1 at the end\n",
    "x_train = x_train.reshape(x_train.shape[0], L, 1) #shape: (8000,60,1 aggiuntiva)\n",
    "x_val =  x_val.reshape(x_val.shape[0], L, 1) #shape: (2000,60,1 aggiuntiva)\n",
    "input_shape = (L, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f534e",
   "metadata": {},
   "source": [
    "### Perform the GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "BATCH_SIZE = 250\n",
    "EPOCHS = 50\n",
    "parameters = {\n",
    "                'reg':['l1', 'l2', 'l1_l2'],\n",
    "                'lmb':[0, 1e-1, 1e-3, 1e-5]\n",
    "             }\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn = build_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=parameters, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77c4c",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The best result was using l1_l2 regularisation function with a D value of 1e-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51436e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"\\n\\nBest: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "      print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab10d0",
   "metadata": {},
   "source": [
    "### Weight Check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d79361",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel = build_model(reg = 'l1_l2', lmb = 1e-5)\n",
    "\n",
    "fit = bestmodel.fit(x_train,y_train,batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0, shuffle=True)\n",
    "\n",
    "Show_weights(bestmodel,0)\n",
    "Show_weights(bestmodel,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
